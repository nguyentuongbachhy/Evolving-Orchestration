# Multi-Agent RL Orchestrator

Há»‡ thá»‘ng Ä‘iá»u phá»‘i Ä‘a tÃ¡c nhÃ¢n dá»±a trÃªn Reinforcement Learning, láº¥y cáº£m há»©ng tá»« mÃ´ hÃ¬nh **Puppeteer** trong nghiÃªn cá»©u "Multi-Agent Collaboration via Evolving Orchestration".

## ğŸ¯ Tá»•ng quan

Dá»± Ã¡n nÃ y chuyá»ƒn Ä‘á»•i cÃ¡c há»‡ thá»‘ng Ä‘a tÃ¡c nhÃ¢n tÄ©nh thÃ nh cÃ¡c orchestrator há»c táº­p Ä‘á»™ng cÃ³ kháº£ nÄƒng:

- **Há»c cÃ¡ch chá»n tÃ¡c nhÃ¢n tá»‘i Æ°u** thÃ´ng qua reinforcement learning
- **Giáº£m chi phÃ­ tÃ­nh toÃ¡n** trong khi duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c
- **ThÃ­ch á»©ng vá»›i cÃ¡c loáº¡i bÃ i toÃ¡n khÃ¡c nhau** má»™t cÃ¡ch tá»± Ä‘á»™ng
- **PhÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh há»£p tÃ¡c** theo thá»i gian

## ğŸ—ï¸ Kiáº¿n trÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RL Orchestrator  â”€â”€â”€â–¶  Policy Network   â”€â”€â”€â”€â–¶ Agent Selection  â”‚
â”‚                 â”‚    â”‚                  â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                       â–²                      â”‚
        â”‚                       â”‚                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reward System  â”‚    â”‚  State Encoder   â”‚    â”‚ Research Agent  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚   Math Agent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Code Agent    â”‚
                                               â”‚  Summary Agent  â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng

```bash
Python 3.8+
PyTorch 2.0+
```

### Thiáº¿t láº­p

```bash
# Clone repository
git clone https://github.com/nguyentuongbachhy/Evolving-Orchestration.git

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng
export OPENAI_API_KEY="your-openai-key"
export TAVILY_API_KEY="your-tavily-key"
```

### Táº¡o file .env

```bash
cp .env.example .env
# Chá»‰nh sá»­a .env vá»›i API keys cá»§a báº¡n
OPENAI_API_KEY=your_openai_key_here
TAVILY_API_KEY=your_tavily_key_here
```

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Cháº¡y toÃ n bá»™ pipeline

```bash
# Cháº¡y thá»±c nghiá»‡m hoÃ n chá»‰nh
python train.py

# Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u trong dataset/ vÃ  checkpoint/
```

### Thá»±c thi tá»«ng bÆ°á»›c

```bash
# BÆ°á»›c 1: Thu tháº­p dá»¯ liá»‡u tá»« original supervisor
python train.py --step 1 --problems math research

# BÆ°á»›c 2: Huáº¥n luyá»‡n RL orchestrator qua imitation learning
python train.py --step 2

# BÆ°á»›c 3: Fine-tuning vá»›i reinforcement learning
python train.py --step 3

# BÆ°á»›c 4: So sÃ¡nh hiá»‡u suáº¥t
python train.py --step 4
```

### Sá»­ dá»¥ng RL Supervisor trá»±c tiáº¿p

```python
from rl_supervisor import execute_task

# Sá»­ dá»¥ng RL supervisor Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n
result, info = execute_task("What is 2**10 + 3 - 2**9?")

print(f"Káº¿t quáº£: {result}")
print(f"Chi phÃ­: {info['cost_stats']['total_cost']}")
print(f"CÃ¡c agent Ä‘Æ°á»£c sá»­ dá»¥ng: {info['cost_stats']['agent_breakdown'].keys()}")
```

### Cháº¡y interactive mode

```bash
# Cháº¡y RL supervisor á»Ÿ cháº¿ Ä‘á»™ tÆ°Æ¡ng tÃ¡c
python rl_supervisor.py
```

## ğŸ”„ CÃ¡c bÆ°á»›c trong Pipeline

### BÆ°á»›c 1: Thu tháº­p dá»¯ liá»‡u (Data Collection)

Thu tháº­p execution traces tá»« original static supervisor:

- **Input**: Test cases (toÃ¡n há»c, nghiÃªn cá»©u, láº­p trÃ¬nh, há»—n há»£p)
- **Output**: `expert_traces.json` vá»›i chuá»—i agents vÃ  chi phÃ­
- **Má»¥c Ä‘Ã­ch**: Táº¡o dá»¯ liá»‡u huáº¥n luyá»‡n cho RL orchestrator

### BÆ°á»›c 2: Imitation Learning

Pre-train RL orchestrator Ä‘á»ƒ báº¯t chÆ°á»›c original supervisor:

- **Input**: Execution traces
- **PhÆ°Æ¡ng phÃ¡p**: Supervised learning (cross-entropy loss)
- **Output**: Pre-trained policy network
- **Má»¥c Ä‘Ã­ch**: Khá»Ÿi táº¡o cho RL training

### BÆ°á»›c 3: RL Fine-tuning

Tá»‘i Æ°u orchestrator sá»­ dá»¥ng thuáº­t toÃ¡n REINFORCE:

- **PhÆ°Æ¡ng phÃ¡p**: Policy gradient vá»›i reward = accuracy - Î»Ã—cost
- **Má»¥c tiÃªu**: Tá»‘i Ä‘a hÃ³a thÃ nh cÃ´ng task vÃ  tá»‘i thiá»ƒu hÃ³a chi phÃ­ tÃ­nh toÃ¡n
- **Output**: RL orchestrator Ä‘Æ°á»£c huáº¥n luyá»‡n hoÃ n chá»‰nh
- **Má»¥c Ä‘Ã­ch**: Há»c cÃ¡c chiáº¿n lÆ°á»£c chá»n agent tá»‘i Æ°u

### BÆ°á»›c 4: Benchmarking

So sÃ¡nh hiá»‡u suáº¥t original vs RL supervisor:

- **Metrics**: Äá»™ chÃ­nh xÃ¡c, thá»i gian thá»±c thi, chi phÃ­, sá»­ dá»¥ng agent
- **PhÃ¢n tÃ­ch**: Ã nghÄ©a thá»‘ng kÃª, tá»· lá»‡ tháº¯ng, cáº£i thiá»‡n hiá»‡u suáº¥t
- **Output**: BÃ¡o cÃ¡o hiá»‡u suáº¥t toÃ n diá»‡n
- **Má»¥c Ä‘Ã­ch**: XÃ¡c thá»±c cáº£i tiáº¿n vÃ  phÃ¢n tÃ­ch hÃ nh vi

## ğŸ¤– CÃ¡c Agent trong há»‡ thá»‘ng

### Research Agent

- **Chá»©c nÄƒng**: Xá»­ lÃ½ cÃ¡c task nghiÃªn cá»©u, tÃ¬m kiáº¿m thÃ´ng tin
- **Tools**: TavilySearch (tÃ¬m kiáº¿m web)
- **á»¨ng dá»¥ng**: Tráº£ lá»i cÃ¢u há»i sá»± kiá»‡n, tÃ¬m kiáº¿m thÃ´ng tin má»›i

### Math Agent

- **Chá»©c nÄƒng**: Xá»­ lÃ½ cÃ¡c phÃ©p tÃ­nh toÃ¡n há»c
- **Tools**: Math expression evaluator
- **á»¨ng dá»¥ng**: Giáº£i cÃ¡c bÃ i toÃ¡n sá»‘ há»c, Ä‘áº¡i sá»‘

### Code Agent

- **Chá»©c nÄƒng**: Xá»­ lÃ½ cÃ¡c task láº­p trÃ¬nh
- **Tools**: Python REPL
- **á»¨ng dá»¥ng**: Viáº¿t vÃ  thá»±c thi code Python, debug

### Summary Agent

- **Chá»©c nÄƒng**: Tá»•ng há»£p káº¿t quáº£ tá»« cÃ¡c agent khÃ¡c
- **Tools**: Result synthesizer
- **á»¨ng dá»¥ng**: Táº¡o cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng, tá»•ng há»£p thÃ´ng tin

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

Dá»±a trÃªn nghiÃªn cá»©u, báº¡n cÃ³ thá»ƒ mong Ä‘á»£i:

- **Äá»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tá»‘t hÆ¡n** (Â±2%)
- **Giáº£m 20-40% chi phÃ­** thÃ´ng qua viá»‡c chá»n agent thÃ´ng minh hÆ¡n
- **Nhanh hÆ¡n 10-30%** qua viá»‡c káº¿t thÃºc sá»›m
- **Sá»­ dá»¥ng agent hiá»‡u quáº£ hÆ¡n**

### Máº«u Output

```json
{
  "summary": {
    "total_tasks": 30,
    "original_avg_accuracy": 0.85,
    "rl_avg_accuracy": 0.87,
    "accuracy_improvement": 0.02,
    "avg_time_improvement": 0.25,
    "avg_cost_improvement": 0.32
  },
  "win_rates": {
    "rl_better_accuracy": 0.6,
    "rl_faster": 0.73,
    "rl_cheaper": 0.8
  },
  "orchestration_metrics": {
    "agent_diversity": 0.75,
    "graph_density": 0.6,
    "reasoning_depth": 3,
    "cycle_count": 0
  }
}
```

## ğŸ“ Cáº¥u trÃºc Project

```
seminar/
â”œâ”€â”€ README.md                   # File hÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ requirements.txt           # Danh sÃ¡ch dependencies
â”œâ”€â”€ .env.example              # Template cho environment variables
â”œâ”€â”€ langgraph.json            # Cáº¥u hÃ¬nh LangGraph
â”œâ”€â”€ train.py                  # Main training pipeline
â”œâ”€â”€ original_supervisor.py    # Original static supervisor
â”œâ”€â”€ rl_supervisor.py         # RL-based supervisor
â”‚
â”œâ”€â”€ orchestration/           # Core RL orchestration logic
â”‚   â”œâ”€â”€ rl_orchestrator.py  # RL orchestrator vá»›i PolicyNetwork
â”‚   â”œâ”€â”€ reward_system.py    # Há»‡ thá»‘ng tÃ­nh reward
â”‚   â””â”€â”€ training_manager.py # Quáº£n lÃ½ RL training
â”‚
â”œâ”€â”€ train/                  # Training modules
â”‚   â”œâ”€â”€ data_collector.py   # Thu tháº­p dá»¯ liá»‡u tá»« original supervisor
â”‚   â”œâ”€â”€ imitation_trainer.py # Imitation learning trainer
â”‚   â””â”€â”€ benchmark_pipeline.py # So sÃ¡nh hiá»‡u suáº¥t
â”‚
â”œâ”€â”€ test/                   # Test cases vÃ  testing utilities
â”‚   â”œâ”€â”€ testcases.py       # Äá»‹nh nghÄ©a cÃ¡c test case
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ utils/                  # Utilities
â”‚   â””â”€â”€ cost_tracker.py    # Theo dÃµi chi phÃ­ API calls
â”‚
â”œâ”€â”€ dataset/               # Generated training data
â”‚   â”œâ”€â”€ expert_traces.json # Expert demonstrations
â”‚   â”œâ”€â”€ collection_stats.json
â”‚   â”œâ”€â”€ training_results.json
â”‚   â””â”€â”€ benchmark_results.json
â”‚
â”œâ”€â”€ checkpoint/            # Model checkpoints
â”‚   â””â”€â”€ orchestrator.pth  # Trained RL orchestrator
â”‚
â””â”€â”€ .langgraph_api/       # LangGraph runtime data
    â””â”€â”€ store.pckl
```

## ğŸ› ï¸ Cáº¥u hÃ¬nh

### Hyperparameters chÃ­nh

```python
# Trong train.py vÃ  cÃ¡c component
LEARNING_RATE = 0.001          # Learning rate cho policy network
LAMBDA_COST = 0.1              # Trá»ng sá»‘ tradeoff cost-accuracy
BATCH_SIZE = 16                # Batch size cho training
NUM_EPOCHS = 100               # Sá»‘ epoch cho imitation learning
RL_EPISODES = 20               # Sá»‘ episode cho RL fine-tuning
HIDDEN_DIM = 256               # KÃ­ch thÆ°á»›c hidden layer
```

### Loáº¡i bÃ i toÃ¡n há»— trá»£

```python
# CÃ¡c loáº¡i problem Ä‘Æ°á»£c há»— trá»£
PROBLEM_TYPES = ["math", "research", "code", "mixed"]

# Trong test/testcases.py:
TestCases.get_math_test_cases()     # BÃ i toÃ¡n tÃ­nh toÃ¡n
TestCases.get_research_test_cases() # CÃ¢u há»i sá»± kiá»‡n
TestCases.get_code_test_cases()     # BÃ i toÃ¡n láº­p trÃ¬nh
TestCases.get_mixed_test_cases()    # BÃ i toÃ¡n Ä‘a bÆ°á»›c
```

### Policy Network Architecture

```python
# Multi-layer vá»›i attention mechanism
class PolicyNetwork(nn.Module):
    - State embedding layer
    - Multi-head attention
    - Agent-specific projection
    - Temperature-controlled softmax
    - Entropy regularization
```

## ğŸ“ˆ Theo dÃµi & PhÃ¢n tÃ­ch

### Theo dÃµi quÃ¡ trÃ¬nh training

```bash
# Xem training logs
tail -f dataset/training_results.json

# PhÃ¢n tÃ­ch káº¿t quáº£
python -c "
import json
with open('dataset/benchmark_results.json') as f:
    results = json.load(f)
print('Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c:', results['comprehensive_report']['summary']['accuracy_improvement'])
print('Giáº£m chi phÃ­:', results['comprehensive_report']['summary']['avg_cost_improvement'])
"
```

### Metrics theo dÃµi

```python
# Orchestration metrics Ä‘Æ°á»£c track:
- Agent diversity: Äa dáº¡ng trong viá»‡c chá»n agent
- Graph density: Máº­t Ä‘á»™ káº¿t ná»‘i giá»¯a cÃ¡c agent
- Reasoning depth: Äá»™ sÃ¢u suy luáº­n
- Cycle count: Sá»‘ lÆ°á»£ng chu ká»³ láº·p
- Agent usage: Thá»‘ng kÃª sá»­ dá»¥ng tá»«ng agent

# Cost metrics:
- Total cost: Tá»•ng chi phÃ­ API
- Agent breakdown: Chi phÃ­ theo tá»«ng agent
- Time efficiency: Hiá»‡u suáº¥t thá»i gian
```

### Visualization (tÃ¹y chá»n)

```python
# Váº½ biá»ƒu Ä‘á»“ training progress
import matplotlib.pyplot as plt
import json

with open('dataset/training_results.json') as f:
    data = json.load(f)

# Plot reward curve náº¿u cÃ³
if 'rl_training' in data:
    rewards = data['rl_training']['episode_rewards']
    plt.plot(rewards)
    plt.title('RL Training Progress')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.show()
```

## ğŸ¤ Má»Ÿ rá»™ng há»‡ thá»‘ng

### ThÃªm Agent má»›i

```python
# Trong rl_supervisor.py
from langchain_core.tools import tool

@tool
def custom_function(input_data: str) -> str:
    """MÃ´ táº£ chá»©c nÄƒng cá»§a tool"""
    # Logic xá»­ lÃ½ cá»§a báº¡n
    return result

# Táº¡o agent má»›i
custom_agent = create_react_agent(
    model="openai:gpt-4o-mini",
    tools=[custom_function],
    prompt=(
        "You are a custom agent. "
        "Assist ONLY custom-related tasks, DO NOT do any else. "
        "After you're done with your tasks, respond to the supervisor directly. "
        "Respond ONLY with the results of your work, do NOT include ANY other text."
    ),
    name="custom_agent"
)

# Cáº­p nháº­t danh sÃ¡ch agents
agents["custom_agent"] = custom_agent

# Cáº­p nháº­t orchestrator
orchestrator = RLOrchestrator(agent_names=list(agents.keys()))
```

### ThÃªm Test Cases má»›i

```python
# Trong test/testcases.py
@staticmethod
def get_custom_test_cases() -> List[Tuple[str, str]]:
    """Custom test cases for your domain"""
    return [
        ("Your custom task", "expected_answer"),
        ("Another custom task", "another_answer"),
        # ThÃªm nhiá»u task hÆ¡n...
    ]

# Cáº­p nháº­t get_all_test_cases()
@staticmethod
def get_all_test_cases() -> Dict[str, List[Tuple[str, str]]]:
    return {
        "math": TestCases.get_math_test_cases(),
        "research": TestCases.get_research_test_cases(),
        "code": TestCases.get_code_test_cases(),
        "mixed": TestCases.get_mixed_test_cases(),
        "custom": TestCases.get_custom_test_cases()  # ThÃªm dÃ²ng nÃ y
    }
```

## ğŸ› Xá»­ lÃ½ sá»± cá»‘

### CÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p

**"KhÃ´ng thu tháº­p Ä‘Æ°á»£c traces"**

- Kiá»ƒm tra API keys Ä‘Ã£ Ä‘Æ°á»£c set Ä‘Ãºng trong .env
- Kiá»ƒm tra original supervisor cháº¡y thÃ nh cÃ´ng
- XÃ¡c minh test cases phÃ¹ há»£p vÃ  Ä‘Ãºng format

**"Äá»™ chÃ­nh xÃ¡c imitation learning tháº¥p"**

- TÄƒng sá»‘ epoch training
- Giáº£m learning rate
- Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u trace
- Äáº£m báº£o dá»¯ liá»‡u Ä‘á»§ Ä‘a dáº¡ng

**"RL khÃ´ng cáº£i thiá»‡n"**

- Äiá»u chá»‰nh tham sá»‘ lambda_cost
- Kiá»ƒm tra reward function
- Kiá»ƒm tra Ä‘a dáº¡ng dá»¯ liá»‡u training
- TÄƒng sá»‘ episode training

**"Benchmark tháº¥t báº¡i"**

- Äáº£m báº£o cáº£ hai supervisor sá»­ dá»¥ng cÃ¹ng API keys
- Kiá»ƒm tra checkpoint file tá»“n táº¡i
- XÃ¡c minh format test case Ä‘Ãºng

**"CUDA out of memory"**

- Giáº£m batch size
- Sá»­ dá»¥ng CPU thay vÃ¬ GPU: `device = "cpu"`
- Giáº£m hidden_dim cá»§a PolicyNetwork

**"API rate limit"**

- ThÃªm delay giá»¯a cÃ¡c API call
- Sá»­ dá»¥ng API key cÃ³ rate limit cao hÆ¡n
- Giáº£m sá»‘ test case Ä‘á»ƒ test

## ï¿½ Chi tiáº¿t ká»¹ thuáº­t

### PolicyNetwork Architecture

```python
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim: int, num_agents: int, hidden_dim: int = 256):
        # Multi-layer architecture vá»›i:
        - State embedding vá»›i LayerNorm vÃ  SiLU activation
        - Multi-head attention mechanism (8 heads)
        - Agent-specific projection layers
        - Temperature-controlled softmax cho exploration
        - Dropout cho regularization
```

### Reward System

```python
# Reward = Task Success - Î» * Cost
reward = success_reward - lambda_cost * normalized_cost

# Vá»›i:
- success_reward: 1.0 náº¿u thÃ nh cÃ´ng, 0.0 náº¿u tháº¥t báº¡i
- lambda_cost: Trá»ng sá»‘ balance accuracy/cost (default: 0.1)
- normalized_cost: Chi phÃ­ Ä‘Æ°á»£c chuáº©n hÃ³a theo baseline
```

### State Representation

```python
# State bao gá»“m:
- Task embedding (tá»« ná»™i dung task)
- History cá»§a agent selections
- Current message context
- Agent usage statistics
- Cost accumulation
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2310.00615)
- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [PyTorch RL Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
- [REINFORCE Algorithm](https://spinningup.openai.com/en/latest/algorithms/vpg.html)

## ğŸ“„ License

MIT License - xem LICENSE file Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ™‹â€â™‚ï¸ Há»— trá»£

Äá»ƒ Ä‘Æ°á»£c há»— trá»£ khi gáº·p váº¥n Ä‘á»:

1. Kiá»ƒm tra pháº§n troubleshooting á»Ÿ trÃªn
2. Xem cÃ¡c issue Ä‘Ã£ cÃ³ trong repository
3. Táº¡o issue má»›i vá»›i error logs chi tiáº¿t
4. Bao gá»“m thÃ´ng tin mÃ´i trÆ°á»ng vÃ  cáº¥u hÃ¬nh

## ğŸ¯ Roadmap

- [ ] ThÃªm há»— trá»£ cho multi-modal agents
- [ ] TÃ­ch há»£p vá»›i cÃ¡c LLM khÃ¡c (Claude, Gemini)
- [ ] Web UI Ä‘á»ƒ monitor training process
- [ ] Distributed training cho datasets lá»›n
- [ ] Advanced reward shaping strategies

---

**ChÃºc báº¡n orchestrating vui váº»! ğŸ­**
